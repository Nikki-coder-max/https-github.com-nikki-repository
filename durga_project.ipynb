{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.docstore.document import Document\n",
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import faiss\n",
    "from sentence_transformers import CrossEncoder, SentenceTransformer\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from peft import LoraConfig, get_peft_model  # LoRA fine-tuning\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract text, images, and tables from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a PDF.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text(\"text\") + \"\\n\"\n",
    "    return text.strip()\n",
    "\n",
    "def extract_images_from_pdf(pdf_path, output_folder):\n",
    "    \"\"\"Extract images from a PDF and save them to a folder.\"\"\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    doc = fitz.open(pdf_path)\n",
    "    image_paths = []\n",
    "    for page_num, page in enumerate(doc):\n",
    "        for img_index, img in enumerate(page.get_images(full=True)):\n",
    "            xref = img[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            img_path = os.path.join(output_folder, f\"page_{page_num+1}_img_{img_index+1}.{base_image['ext']}\")\n",
    "            with open(img_path, \"wb\") as f:\n",
    "                f.write(base_image[\"image\"])\n",
    "            image_paths.append(img_path)\n",
    "    return image_paths\n",
    "\n",
    "def perform_ocr_on_images(image_paths):\n",
    "    \"\"\"Perform OCR on extracted images.\"\"\"\n",
    "    ocr_texts = []\n",
    "    for img_path in image_paths:\n",
    "        try:\n",
    "            image = Image.open(img_path)\n",
    "            text = pytesseract.image_to_string(image).strip()\n",
    "            if text:\n",
    "                ocr_texts.append(text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path}: {e}\")\n",
    "    return ocr_texts\n",
    "\n",
    "def extract_tables_from_pdf(pdf_path):\n",
    "    \"\"\"Extract tables from a PDF.\"\"\"\n",
    "    table_texts = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_num, page in enumerate(pdf.pages, start=1):\n",
    "            tables = page.extract_tables()\n",
    "            for table_index, table in enumerate(tables):\n",
    "                df = pd.DataFrame(table)\n",
    "                table_texts.append(f\"Table {table_index + 1} on Page {page_num}:\\n{df.to_string(index=False, header=False)}\\n\")\n",
    "    return table_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "pdf_path = r\"C:\\Users\\user\\Downloads\\health_care\\Python_Durga.pdf\"\n",
    "output_folder = r\"C:\\Users\\user\\Downloads\\health_care\\output_image\"\n",
    "# Extract text, images, and tables\n",
    "text = extract_text_from_pdf(pdf_path)\n",
    "image_paths = extract_images_from_pdf(pdf_path, output_folder)\n",
    "ocr_texts = perform_ocr_on_images(image_paths)\n",
    "table_texts = extract_tables_from_pdf(pdf_path)\n",
    "\n",
    "# Combine all text data\n",
    "all_text = text + \"\\n\".join(ocr_texts) + \"\\n\".join(table_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Clean and chunk text\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and normalize text.\"\"\"\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)  # Replace multiple newlines\n",
    "    text = re.sub(r' {2,}', ' ', text)  # Remove extra spaces\n",
    "    text = re.sub(r'(?<=\\w)-\\n(?=\\w)', '', text)  # Fix hyphenated words\n",
    "    text = text.replace(\"\\n\\n\", \" \").replace(\"\\n\", \" \")  # Fix broken lines\n",
    "    text = re.sub(r' {2,}', ' ', text)  # Remove excessive spaces\n",
    "    text = re.sub(r'(?<=\\w)-\\n(?=\\w)', '', text)  # Fix hyphenated word breaks\n",
    "    text = text.replace(\"\\n\\n\", \" \")  # Fix broken line splits\n",
    "    text = text.replace(\"\\n\", \" \")  # Fix remaining broken lines\n",
    "    return text.strip()\n",
    "\n",
    "def chunk_text(text, chunk_size=600, overlap=100):\n",
    "    \"\"\"Split text into smaller chunks.\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=overlap,\n",
    "        separators=[\"\\n\\n\", \"?\", \".\", \"!\", \"\\n\", \" \"]\n",
    "    )\n",
    "    return text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Clean and chunk the text\n",
    "cleaned_text = clean_text(all_text)\n",
    "text_chunks = chunk_text(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.base import Embeddings\n",
    "from typing import List\n",
    "\n",
    "# Custom embeddings class for the fine-tuned model\n",
    "class FineTunedEmbeddings(Embeddings):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Embed a list of documents using the fine-tuned model.\"\"\"\n",
    "        return self.model.encode(texts).tolist()\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        \"\"\"Embed a single query using the fine-tuned model.\"\"\"\n",
    "        return self.model.encode([text]).tolist()[0]\n",
    "\n",
    "# Step 3: Fine-tune the embedding model using LoRA\n",
    "def fine_tune_embedding_model(model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "    \"\"\"Fine-tune the embedding model using LoRA.\"\"\"\n",
    "    model = SentenceTransformer(model_name)\n",
    "    \n",
    "    # Define LoRA configuration\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,  # Rank of the low-rank adaptation\n",
    "        lora_alpha=16,  # Scaling factor\n",
    "        target_modules=[\"key\", \"value\"],  # Target modules for adaptation\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\"\n",
    "    )\n",
    "    \n",
    "    # Apply LoRA to the model\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    # Wrap the fine-tuned model in the custom embeddings class\n",
    "    embeddings_model = FineTunedEmbeddings(model)\n",
    "    return embeddings_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the embedding model\n",
    "embeddings_model = fine_tune_embedding_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create Pinecone index and store embeddings\n",
    "def create_pinecone_index(index_name, dimension=384):\n",
    "    \"\"\"Create a Pinecone index.\"\"\"\n",
    "    os.environ[\"PINECONE_API_KEY\"] = \"pcsk_EggKG_S8oLWsWYLJmNNmoFyoutuZ33RBcNoCuqjtGSr9KUumYZbTuUMZj7feE1MGUNGUG\"\n",
    "    pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=dimension,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "    print(f\"✅ Pinecone index '{index_name}' created successfully!\")\n",
    "\n",
    "def check_index_exists(index_name):\n",
    "    \"\"\"Check if a Pinecone index exists.\"\"\"\n",
    "    os.environ[\"PINECONE_API_KEY\"] = \"pcsk_EggKG_S8oLWsWYLJmNNmoFyoutuZ33RBcNoCuqjtGSr9KUumYZbTuUMZj7feE1MGUNGUG\"\n",
    "    pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "    existing_indexes = pc.list_indexes()\n",
    "    return index_name in existing_indexes\n",
    "\n",
    "def store_embeddings_in_pinecone(text_chunks, embedding_model, index_name):\n",
    "    \"\"\"Store embeddings in Pinecone.\"\"\"\n",
    "    documents = [Document(page_content=chunk) for chunk in text_chunks]\n",
    "    docsearch = PineconeVectorStore.from_documents(\n",
    "        documents=documents,\n",
    "        index_name=index_name,\n",
    "        embedding=embedding_model\n",
    "    )\n",
    "    return docsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pinecone index and store embeddings\n",
    "index_name = \"durga-one\"\n",
    "if not check_index_exists(index_name):\n",
    "    create_pinecone_index(index_name)\n",
    "else:\n",
    "    print(f\"Index '{index_name}' already exists. Skipping creation.\")\n",
    "docsearch = store_embeddings_in_pinecone(text_chunks, embeddings_model, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import faiss\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Step 5: Multi-queue retrieval with BM25, dense embeddings, and HYDE\n",
    "def multi_queue_retrieval(query, bm25, embedder, retrieved_docs, top_k=5):\n",
    "    \"\"\"Perform multi-queue retrieval using BM25, dense embeddings, and HYDE.\"\"\"\n",
    "    retrieved_texts = [doc.page_content for doc in retrieved_docs]\n",
    "    \n",
    "    # BM25 scores\n",
    "    bm25_scores = bm25.get_scores(word_tokenize(query.lower()))\n",
    "    \n",
    "    # Dense embeddings\n",
    "    doc_embeddings = np.array(embedder.encode(retrieved_texts))\n",
    "    query_embedding = embedder.encode(query).reshape(1, -1)\n",
    "    \n",
    "    # FAISS index for dense retrieval\n",
    "    index = faiss.IndexFlatL2(doc_embeddings.shape[1])\n",
    "    index.add(doc_embeddings)\n",
    "    _, dense_indices = index.search(query_embedding, k=len(retrieved_texts))\n",
    "    dense_scores = np.exp(-dense_indices[0])  # Convert distances to scores\n",
    "    \n",
    "    # HYDE: Generate hypothetical document embeddings\n",
    "    hyde_prompt = f\"Generate a hypothetical document that answers the question: {query}\"\n",
    "    hyde_doc = embedder.encode(hyde_prompt).reshape(1, -1)\n",
    "    _, hyde_indices = index.search(hyde_doc, k=len(retrieved_texts))\n",
    "    hyde_scores = np.exp(-hyde_indices[0])\n",
    "    \n",
    "    # Normalize scores\n",
    "    scaler = MinMaxScaler()\n",
    "    bm25_scores_scaled = scaler.fit_transform(np.array(bm25_scores).reshape(-1, 1)).flatten()\n",
    "    dense_scores_scaled = scaler.fit_transform(np.array(dense_scores).reshape(-1, 1)).flatten()\n",
    "    hyde_scores_scaled = scaler.fit_transform(np.array(hyde_scores).reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Combine scores\n",
    "    hybrid_scores = 0.4 * bm25_scores_scaled + 0.4 * dense_scores_scaled + 0.2 * hyde_scores_scaled\n",
    "    \n",
    "    # Sort documents by hybrid scores\n",
    "    return [doc for doc, _ in sorted(zip(retrieved_docs, hybrid_scores), key=lambda x: x[1], reverse=True)[:top_k]]\n",
    "\n",
    "# Initialize the embedding model for multi-queue retrieval\n",
    "try:\n",
    "    # Use the correct model name\n",
    "    embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading the embedding model: {e}\")\n",
    "    # Fallback to a different model if the primary one fails\n",
    "    embedder = SentenceTransformer(\"sentence-transformers/paraphrase-MiniLM-L6-v2\")\n",
    "\n",
    "# Perform the initial retrieval\n",
    "query = \"write a program to add two numbers with list comprehension?\"\n",
    "retriever = docsearch.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 15, \"fetch_k\": 50, \"lambda_mult\": 0.7})\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "\n",
    "# Initialize BM25 with the retrieved documents\n",
    "bm25 = BM25Okapi([word_tokenize(doc.page_content.lower()) for doc in retrieved_docs])\n",
    "\n",
    "# Perform multi-queue retrieval\n",
    "top_docs = multi_queue_retrieval(query, bm25, embedder, retrieved_docs)\n",
    "\n",
    "# Print the top documents\n",
    "for i, doc in enumerate(top_docs):\n",
    "    print(f\"Top {i+1} Document: {doc.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Rerank documents using a cross-encoder\n",
    "def rerank_documents(query, retrieved_docs):\n",
    "    \"\"\"Rerank documents using a cross-encoder.\"\"\"\n",
    "    reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "    pairs = [(query, doc.page_content) for doc in retrieved_docs]\n",
    "    scores = reranker.predict(pairs)\n",
    "    ranked_indices = np.argsort(scores)[::-1]\n",
    "    return [retrieved_docs[i] for i in ranked_indices[:2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranked_docs = rerank_documents(query, top_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import InputExample, losses, SentenceTransformer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ✅ **1. Ensure Model is Available**\n",
    "reranker_name = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "\n",
    "try:\n",
    "    reranker = SentenceTransformer(reranker_name)  # Load pretrained reranker\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Error loading model: {e}\")\n",
    "    print(\"📥 Downloading model manually...\")\n",
    "    reranker = SentenceTransformer.from_pretrained(reranker_name)  # Force download\n",
    "\n",
    "# ✅ **2. Prepare the dataset**\n",
    "train_examples = [\n",
    "    InputExample(texts=[\"What is Python?\", \"Python is a high-level programming language.\"], label=1.0),\n",
    "    InputExample(texts=[\"How do you create a list in Python?\", \"You can create a list using square brackets.\"], label=1.0),\n",
    "    InputExample(texts=[\"What is Python?\", \"Java is an object-oriented programming language.\"], label=0.0),\n",
    "    # Add more examples\n",
    "]\n",
    "\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "\n",
    "# ✅ **3. Define Loss Function**\n",
    "train_loss = losses.CosineSimilarityLoss(reranker)\n",
    "\n",
    "# ✅ **4. Fine-Tune the Reranker**\n",
    "reranker.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=3,\n",
    "    warmup_steps=100,\n",
    "    output_path=\"./reranker-finetuned\"\n",
    ")\n",
    "\n",
    "print(\"✅ Reranker Fine-Tuning Complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🚀 Install required libraries\n",
    "# pip install transformers datasets peft accelerate torch\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    ")\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "#  **1. Load Tokenizer & Base LLM**\n",
    "model_name = \"google/flan-t5-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "#  **2. Automatically Detect CPU or CUDA**\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "#  **3. Load Model with Correct Precision**\n",
    "if device == \"cuda\":\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        model_name, torch_dtype=torch.float16, device_map=\"auto\"\n",
    "    )\n",
    "else:\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "#  **4. Apply LoRA Fine-Tuning Configuration**\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # Low-rank adaptation size\n",
    "    lora_alpha=16,  # Scaling factor\n",
    "    target_modules=[\"q\", \"v\"],  # Apply LoRA to attention layers\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "# **Convert model to LoRA fine-tuning**\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "#  **5. Prepare Training & Validation Dataset**\n",
    "data = [\n",
    "    {\"input_text\": \"What is Python?\", \"output_text\": \"Python is a high-level programming language.\"},\n",
    "    {\"input_text\": \"How do you create a list in Python?\", \"output_text\": \"You can create a list using square brackets, like `my_list = [1, 2, 3]`.\"},\n",
    "    {\"input_text\": \"What is recursion in Python?\", \"output_text\": \"Recursion is when a function calls itself until it reaches a base case.\"},\n",
    "]\n",
    "\n",
    "# **Split dataset into train & eval (80% train, 20% eval)**\n",
    "train_data = data[:2]  \n",
    "eval_data = data[2:]  \n",
    "\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"input_text\": [d[\"input_text\"] for d in train_data],\n",
    "    \"output_text\": [d[\"output_text\"] for d in train_data]\n",
    "})\n",
    "\n",
    "eval_dataset = Dataset.from_dict({\n",
    "    \"input_text\": [d[\"input_text\"] for d in eval_data],\n",
    "    \"output_text\": [d[\"output_text\"] for d in eval_data]\n",
    "})\n",
    "\n",
    "#  **6. Tokenize the Dataset**\n",
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(examples[\"input_text\"], max_length=512, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    targets = tokenizer(examples[\"output_text\"], max_length=512, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_eval_dataset = eval_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "#  **7. Fine-Tune the Model with LoRA**\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./flan-t5-lora\",\n",
    "    per_device_train_batch_size=1,  # Reduced batch size to avoid OOM errors\n",
    "    gradient_accumulation_steps=8,  # Simulate a larger batch size\n",
    "    num_train_epochs=3,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"steps\",  # Enable evaluation after every `eval_steps`\n",
    "    eval_steps=100,  # Run evaluation every 100 steps\n",
    "    predict_with_generate=True,\n",
    "    fp16=(device == \"cuda\"),  # Enable fp16 for GPU only\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_eval_dataset,  # Provided eval dataset\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
    ")\n",
    "\n",
    "#  **8. Train the Model**\n",
    "trainer.train()\n",
    "\n",
    "#  **9. Save the Fine-Tuned Model**\n",
    "trainer.save_model(\"./flan-t5-lora-finetuned\")\n",
    "print(\" Model Fine-Tuned with LoRA and Saved Successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_pinecone import PineconeVectorStore  # Import Pinecone Retriever\n",
    "from sentence_transformers import CrossEncoder, SentenceTransformer  # Import SentenceTransformer for embeddings\n",
    "from typing import List\n",
    "import os\n",
    "\n",
    "# Custom wrapper for SentenceTransformer to provide `embed_query` method\n",
    "class SentenceTransformerEmbeddings:\n",
    "    def __init__(self, model_name: str):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return self.model.encode(text).tolist()\n",
    "\n",
    "# Load the fine-tuned LLM\n",
    "fine_tuned_llm = AutoModelForSeq2SeqLM.from_pretrained(\"./flan-t5-lora-finetuned\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "text_generator = pipeline(\"text2text-generation\", model=fine_tuned_llm, tokenizer=tokenizer)\n",
    "llm = HuggingFacePipeline(pipeline=text_generator)\n",
    "\n",
    "# Load the fine-tuned reranker\n",
    "fine_tuned_reranker = CrossEncoder(\"./reranker-finetuned\")\n",
    "\n",
    "# Load a separate embedding model for Pinecone\n",
    "embedding_model = SentenceTransformerEmbeddings('all-MiniLM-L6-v2')  # Use the custom wrapper\n",
    "\n",
    "# Ensure Pinecone API Key is Set\n",
    "os.environ[\"PINECONE_API_KEY\"] = \"pcsk_EggKG_S8oLWsWYLJmNNmoFyoutuZ33RBcNoCuqjtGSr9KUumYZbTuUMZj7feE1MGUNGUG\"\n",
    "\n",
    "# Initialize Pinecone Retriever\n",
    "index_name = \"durga-update\"  # Replace with your index name\n",
    "retriever = PineconeVectorStore.from_existing_index(index_name=index_name, embedding=embedding_model)\n",
    "\n",
    "# Define the prompt template\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"query\"],\n",
    "    template=\"\"\"You are an AI assistant that answers Python programming questions using retrieved context.\n",
    "    - Think step-by-step using logical reasoning (Chain-of-Thought).\n",
    "    - Use relevant examples when needed.\n",
    "    - Prioritize clarity and accuracy.\n",
    "\n",
    "    Context: {context}\n",
    "    Question: {query}\n",
    "    Answer: Let's think step-by-step:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Generate response using the fine-tuned LLM\n",
    "def generate_response(query: str, reranked_docs: List[Document]) -> str:\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in reranked_docs])\n",
    "    \n",
    "    # Format the prompt correctly\n",
    "    prompt_text = prompt_template.format(context=context, query=query)\n",
    "    \n",
    "    # Run LLM Chain\n",
    "    llm_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "    response = llm_chain.run({\"context\": context, \"query\": query})\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Rerank documents using the fine-tuned reranker\n",
    "def rerank_documents(query: str, retrieved_docs: List[Document]) -> List[Document]:\n",
    "    pairs = [(query, doc.page_content) for doc in retrieved_docs]\n",
    "    scores = fine_tuned_reranker.predict(pairs)\n",
    "    ranked_indices = np.argsort(scores)[::-1]  # Sort in descending order\n",
    "    return [retrieved_docs[i] for i in ranked_indices[:2]]  # Select top 2\n",
    "\n",
    "# Example usage\n",
    "query = \"write a program to add two numbers with list comprehension?\"\n",
    "retrieved_docs = retriever.as_retriever().invoke(query)  # Fixed `retriever`\n",
    "reranked_docs = rerank_documents(query, retrieved_docs)\n",
    "response = generate_response(query, reranked_docs)\n",
    "\n",
    "print(\"🤖 AI Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from langchain import PromptTemplate, LLMChain, HuggingFacePipeline\n",
    "from typing import List\n",
    "\n",
    "def chunk_text(text, max_tokens=400):\n",
    "    \"\"\"Split text into chunks that fit within the model's token limit.\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for word in words:\n",
    "        # Approximate token count (1 word ≈ 1.3 tokens)\n",
    "        word_length = len(word) + 1  # Add 1 for the space\n",
    "        if current_length + word_length > max_tokens:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "        current_chunk.append(word)\n",
    "        current_length += word_length\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "    return chunks\n",
    "\n",
    "def generate_response(query: str, reranked_docs: List[Document]) -> str:\n",
    "    \"\"\"Generate a response using an LLM with few-shot examples and CoT reasoning.\"\"\"\n",
    "    model_name = \"google/flan-t5-large\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    text_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "    llm = HuggingFacePipeline(pipeline=text_generator)\n",
    "\n",
    "    # Define few-shot examples with CoT reasoning\n",
    "    few_shot_examples = \"\"\"\n",
    "    Example 1:\n",
    "    Context: Python is a high-level programming language.\n",
    "    Q: What is Python?\n",
    "    A: Let's think step-by-step:\n",
    "       1. Python is described as a \"high-level programming language.\"\n",
    "       2. High-level languages are known for their simplicity and readability.\n",
    "       3. Python is widely used in web development, data analysis, and AI.\n",
    "       Therefore, Python is a high-level programming language known for its simplicity and readability, widely used in web development, data analysis, and AI.\n",
    "\n",
    "    Example 2:\n",
    "    Context: Lists in Python are ordered, mutable collections of items.\n",
    "    Q: How do you create a list in Python?\n",
    "    A: Let's think step-by-step:\n",
    "       1. Lists in Python are created using square brackets `[]`.\n",
    "       2. Items are separated by commas.\n",
    "       3. For example, `my_list = [1, 2, 3]` creates a list with three integers.\n",
    "       Therefore, you can create a list in Python using square brackets, like `my_list = [1, 2, 3]`.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the prompt template with few-shot examples and CoT reasoning\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"context\", \"query\"],\n",
    "        template=f\"\"\"You are an AI assistant that answers Python programming questions using retrieved context.\n",
    "        - Think step-by-step using logical reasoning (Chain-of-Thought).\n",
    "        - Use relevant examples when needed.\n",
    "        - Prioritize clarity and accuracy.\n",
    "\n",
    "        Here are some examples of how to answer Python programming questions:\n",
    "        {few_shot_examples}\n",
    "\n",
    "        Now, answer the following Python programming question using the provided context:\n",
    "        Context: {{context}}\n",
    "        Question: {{query}}\n",
    "        Answer: Let's think step-by-step:\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # Prepare the context by extracting page_content from each Document\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in reranked_docs])\n",
    "    context_chunks = chunk_text(context, max_tokens=400)\n",
    "    responses = []\n",
    "\n",
    "    for chunk in context_chunks:\n",
    "        # Format the prompt with the chunk and query\n",
    "        prompt_text = prompt_template.format(context=chunk, query=query)\n",
    "        \n",
    "        # Generate the response using LLM chaining\n",
    "        llm_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "        response = llm_chain.run({\"context\": chunk, \"query\": query})\n",
    "        responses.append(response)\n",
    "\n",
    "    # Combine responses from all chunks\n",
    "    combined_response = \" \".join(responses)\n",
    "\n",
    "    # Post-process the response to remove unwanted phrases\n",
    "    cleaned_response = combined_response.replace(\"If unsure, say 'I don't know'.\", \"\").strip()\n",
    "    return cleaned_response\n",
    "\n",
    "# Example usage\n",
    "response = generate_response(query, reranked_docs)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the Serper API key as an environment variable\n",
    "os.environ[\"SERPER_API_KEY\"] = \"e8a743391241752aabd30ddf8b2ef0928534b43f\"\n",
    "\n",
    "# Initialize the GoogleSerperAPIWrapper\n",
    "google_search = GoogleSerperAPIWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, Tool, AgentType\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.utilities import GoogleSerperAPIWrapper, WikipediaAPIWrapper\n",
    "from langchain_experimental.tools import PythonREPLTool\n",
    "from langchain.chains import LLMMathChain\n",
    "from typing import List, Dict\n",
    "from langchain.docstore.document import Document\n",
    "import os\n",
    "\n",
    "# ✅ **Set API Key for Google Search**\n",
    "os.environ[\"SERPER_API_KEY\"] = \"e8a743391241752aabd30ddf8b2ef0928534b43f\"\n",
    "\n",
    "# ✅ **Custom tool for retrieval**\n",
    "class RetrievalTool(BaseTool):\n",
    "    name: str = \"retrieval_tool\"\n",
    "    description: str = \"Retrieve relevant documents from Pinecone based on the query.\"\n",
    "\n",
    "    def _run(self, tool_input: str) -> List[Document]:  # Expecting a **single query string**\n",
    "        \"\"\"Retrieve documents using Pinecone.\"\"\"\n",
    "        retrieved_docs = retriever.as_retriever().invoke(tool_input)\n",
    "        return retrieved_docs\n",
    "\n",
    "    def _arun(self, tool_input: str):\n",
    "        raise NotImplementedError(\"Async not supported\")\n",
    "\n",
    "# ✅ **Custom tool for reranking**\n",
    "class RerankTool(BaseTool):\n",
    "    name: str = \"rerank_tool\"\n",
    "    description: str = \"Rerank retrieved documents using a fine-tuned cross-encoder.\"\n",
    "\n",
    "    def _run(self, tool_input: Dict) -> List[Document]:  \n",
    "        \"\"\"Rerank documents using the fine-tuned reranker.\"\"\"\n",
    "        if not isinstance(tool_input, dict):\n",
    "            raise TypeError(f\"Expected dictionary input, got {type(tool_input)}\")\n",
    "        \n",
    "        query = tool_input.get(\"query\", \"\")\n",
    "        retrieved_docs = tool_input.get(\"retrieved_docs\", [])\n",
    "        \n",
    "        if not query or not retrieved_docs:\n",
    "            raise ValueError(\"Both 'query' and 'retrieved_docs' must be provided.\")\n",
    "        \n",
    "        return rerank_documents(query, retrieved_docs)\n",
    "\n",
    "    def _arun(self, tool_input: Dict):\n",
    "        raise NotImplementedError(\"Async not supported\")\n",
    "\n",
    "# ✅ **Custom tool for response generation**\n",
    "class ResponseGenerationTool(BaseTool):\n",
    "    name: str = \"response_generation_tool\"\n",
    "    description: str = \"Generate a response using the fine-tuned LLM with Chain-of-Thought reasoning.\"\n",
    "\n",
    "    def _run(self, tool_input: Dict) -> str:\n",
    "        \"\"\"Generate a response using the fine-tuned LLM.\"\"\"\n",
    "        if not isinstance(tool_input, dict):\n",
    "            raise TypeError(f\"Expected dictionary input, got {type(tool_input)}\")\n",
    "        \n",
    "        query = tool_input.get(\"query\", \"\")\n",
    "        reranked_docs = tool_input.get(\"reranked_docs\", [])\n",
    "        \n",
    "        if not query or not reranked_docs:\n",
    "            raise ValueError(\"Both 'query' and 'reranked_docs' must be provided.\")\n",
    "        \n",
    "        return generate_response(query, reranked_docs)\n",
    "\n",
    "    def _arun(self, tool_input: Dict):\n",
    "        raise NotImplementedError(\"Async not supported\")\n",
    "\n",
    "# ✅ **Initialize Tools**\n",
    "retrieval_tool = RetrievalTool()\n",
    "rerank_tool = RerankTool()\n",
    "response_generation_tool = ResponseGenerationTool()\n",
    "\n",
    "# ✅ **Load built-in tools**\n",
    "google_search = GoogleSerperAPIWrapper()\n",
    "wikipedia = WikipediaAPIWrapper()\n",
    "python_repl = PythonREPLTool()\n",
    "llm_math = LLMMathChain(llm=llm)\n",
    "\n",
    "# ✅ **Define built-in tools**\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Google Search\",\n",
    "        func=google_search.run,\n",
    "        description=\"Search Google for real-time information.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Wikipedia\",\n",
    "        func=wikipedia.run,\n",
    "        description=\"Fetch general knowledge from Wikipedia.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Python REPL\",\n",
    "        func=python_repl.run,\n",
    "        description=\"Execute Python code snippets.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Calculator\",\n",
    "        func=llm_math.run,\n",
    "        description=\"Perform complex mathematical calculations.\"\n",
    "    ),\n",
    "    retrieval_tool,\n",
    "    rerank_tool,\n",
    "    response_generation_tool\n",
    "]\n",
    "\n",
    "# ✅ **Initialize the agent**\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,  # Use the fine-tuned LLM\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# ✅ **Example usage**\n",
    "query = \"write a program to add two numbers with list comprehension?\"\n",
    "\n",
    "# ✅ **Call tools properly with dictionary inputs**\n",
    "retrieved_docs = retrieval_tool.run(query)\n",
    "reranked_docs = rerank_tool.run({\"query\": query, \"retrieved_docs\": retrieved_docs})  # ✅ FIXED\n",
    "response = response_generation_tool.run({\"query\": query, \"reranked_docs\": reranked_docs})  # ✅ FIXED\n",
    "\n",
    "print(\"🤖 AI Response:\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
